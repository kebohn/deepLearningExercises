{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "about-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "import numpy\n",
    "import collections\n",
    "import types\n",
    "from scipy.spatial import distance\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86065b48-4e07-423b-a2db-121f965d617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enrolling Gallery...\n",
      "Scoring...\n",
      "Recognized  8 of 14 faces ( 57.14%) with centerlight\n",
      "Recognized 10 of 15 faces ( 66.67%) with leftlight\n",
      "Recognized 15 of 15 faces (100.00%) with normal\n",
      "Recognized 13 of 15 faces ( 86.67%) with noglasses\n",
      "Recognized 14 of 15 faces ( 93.33%) with happy\n",
      "Recognized 14 of 15 faces ( 93.33%) with wink\n",
      "Recognized 15 of 15 faces (100.00%) with sleepy\n",
      "Recognized 15 of 15 faces (100.00%) with surprised\n",
      "Recognized  8 of 15 faces ( 53.33%) with rightlight\n",
      "Recognized 14 of 15 faces ( 93.33%) with sad\n",
      "Recognized  9 of 15 faces ( 60.00%) with glasses\n",
      "\n",
      "Accuracy: 135 of 164 faces ( 82.32%)\n"
     ]
    }
   ],
   "source": [
    "def _forward_impl(self, x): # overwrite forward function from resnet-18\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.maxpool(x)\n",
    "    \n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "    \n",
    "    x = self.avgpool(x)\n",
    "    x = torch.flatten(x,1)\n",
    "    # x = self.fc(x) skip this step!!\n",
    "    \n",
    "    return x\n",
    "\n",
    "network = torchvision.models.resnet18(pretrained=True) # load pretrained model resnet-18\n",
    "network.eval()\n",
    "network._forward_impl = types.MethodType(_forward_impl, network) # overwrite forward function to skip fully connected layer\n",
    "\n",
    "# define image transformations\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(300),\n",
    "    torchvision.transforms.CenterCrop(224), # take only center from image\n",
    "    torchvision.transforms.ToTensor(), # PIL image to tensor\n",
    "    lambda x : x.repeat(3,1,1), # model expects RGB layers, as we have only Grayscale images we copy it such that we have 3 Channels with the same Image\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    lambda x : x.unsqueeze(0) # required by pytorch (add batch dimension)\n",
    "])\n",
    "\n",
    "def extract(path):\n",
    "    with torch.no_grad():\n",
    "        image = PIL.Image.open(path) # load image\n",
    "        tensor = transform(image) # apply transformation defined above\n",
    "        feature = network(tensor)\n",
    "        return feature.numpy().flatten() # convert to a 1-dimensional vector\n",
    "    \n",
    "# load gallery\n",
    "print(\"Enrolling Gallery...\")\n",
    "gallery = {}\n",
    "for index in range(1,16):\n",
    "    subject = F\"subject{index:02d}\" # get subject info\n",
    "    gallery[subject] = extract(F\"yalefaces/{subject}.normal\") # add the normal positions of each face to the gallery\n",
    "    \n",
    "\n",
    "print(\"Scoring...\")\n",
    "variations = collections.defaultdict(lambda: [0,0]) # store number of correct identifiactions  and total number of indetifications per variation\n",
    "for filename in os.listdir(\"yalefaces\"):\n",
    "    probe_subject, variation = os.path.splitext(filename) # split subject and filename extension\n",
    "    path = os.path.join(\"yalefaces\", filename) # create filepath\n",
    "    \n",
    "    if not os.path.isfile(path): # ignore directories\n",
    "        continue\n",
    "    \n",
    "    probe_feature = extract(path) # apply transformations\n",
    " \n",
    "    min_distance = 1e8 # set to high value in the beginning\n",
    "    for gallery_subject, gallery_feature in gallery.items(): # iterate over the defined gallery\n",
    "        dist = distance.euclidean(gallery_feature, probe_feature) # compute euclidean distance between gallery feature and probe feature\n",
    "        if dist < min_distance: # check if distance is smaller, if true set it to best subject\n",
    "            min_distance = dist\n",
    "            best_subject = gallery_subject\n",
    "    \n",
    "    # check if face was recognized for each variation\n",
    "    variations[variation][0] += best_subject == probe_subject # we increase by one if the subject was correctly identified\n",
    "    variations[variation][1] += 1 # we increase this after each iteration such that we have a total number\n",
    "    \n",
    "# print rates for each variation\n",
    "for variation, results in variations.items():\n",
    "    print(F\"Recognized {results[0]:2} of {results[1]} faces ({100 * results[0] / results[1]:6.2f}%) with {variation[1:]}\")\n",
    "    \n",
    "# print total accuracy\n",
    "total = numpy.sum(list(variations.values()), axis=0)\n",
    "print(F\"\\nAccuracy: {total[0]:3} of {total[1]} faces ({100 * total[0] / total[1]:6.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
