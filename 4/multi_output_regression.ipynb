{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "packed-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "inside-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + numpy.exp(-x))\n",
    "\n",
    "def loss_function(Y, T):\n",
    "    return numpy.mean((Y - T) ** 2)\n",
    "                                                   \n",
    "def forward(X, W1, W2):\n",
    "    H = sigmoid(W1.dot(X))\n",
    "    H[0,:] = 1. # bias\n",
    "    return W2.dot(H), H\n",
    "\n",
    "def gradient_layer_1(X, W, H, Y, T):\n",
    "    tmp = W.T.dot(Y - T) * H * (1 - H)\n",
    "    return tmp.dot(X.T) / X.shape[1]\n",
    "\n",
    "def gradient_layer_2(H, Y, T):\n",
    "    return (Y-T).dot(H.T) / Y.shape[1]\n",
    "\n",
    "def descent(X, T, W1, W2, eta):\n",
    "    Y, H = forward(X, W1, W2)\n",
    "    loss = loss_function(Y, T)\n",
    "    W1 -= eta * gradient_layer_1(X, W2, H, Y, T)\n",
    "    W2 -= eta * gradient_layer_2(H, Y, T)\n",
    "    return W1, W2, loss \n",
    "                             \n",
    "def batch(X, T, B):\n",
    "    D = numpy.vstack((X, T))\n",
    "    numpy.random.shuffle(D.T)\n",
    "    for i in range(0, X.shape[1], B):\n",
    "        sample = D[:,i:i+B]\n",
    "        if (sample.shape[1] == B): # ignore the rest if X mod != 0\n",
    "            yield sample[:-3], sample[-3:]\n",
    "            \n",
    "def stochastic_gradient_descent(X, T, W1, W2, B, eta, epochs):\n",
    "    loss_arr = []\n",
    "    for epoch in range(epochs):\n",
    "        batches = batch(X, T, B)\n",
    "        for b in batches: # call the generator, returns a tuple with X and T values\n",
    "            W1, W2, J = descent(b[0], b[1], numpy.copy(W1), numpy.copy(W2), eta)\n",
    "        loss_arr.append(J)\n",
    "    return W1, W2, loss_arr\n",
    "\n",
    "def gradient_descent(X, T, W1, W2, eta, epochs):\n",
    "    loss_arr = []\n",
    "    for epoch in range(epochs):\n",
    "        W1, W2, J = descent(X, T, numpy.copy(W1), numpy.copy(W2), eta)\n",
    "        loss_arr.append(J)\n",
    "    return W1, W2, loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "monthly-ideal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: prepare dataset\n",
    "df = pandas.read_csv(\"data/student-mat.csv\", sep=\";\")\n",
    "drop_columns = [\"Mjob\", \"Fjob\", \"reason\", \"guardian\"]\n",
    "df.drop(drop_columns, inplace=True, axis=1)\n",
    "\n",
    "# convert to binary\n",
    "df[\"school\"].replace({\"GP\": -1, \"MS\": 1}, inplace=True)\n",
    "df[\"sex\"].replace({\"F\": -1, \"M\": 1}, inplace=True)\n",
    "df[\"address\"].replace({\"U\": -1, \"R\": 1}, inplace=True)\n",
    "df[\"famsize\"].replace({\"GT3\": -1, \"LE3\": 1}, inplace=True)\n",
    "df[\"Pstatus\"].replace({\"A\": -1, \"T\": 1}, inplace=True)\n",
    "df[\"schoolsup\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "df[\"famsup\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "df[\"paid\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "df[\"activities\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "df[\"nursery\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "df[\"higher\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "df[\"internet\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "df[\"romantic\"].replace({\"no\": -1, \"yes\": 1}, inplace=True)\n",
    "\n",
    "# convert all values to float\n",
    "df = df.astype('float128')\n",
    "\n",
    "# init input & target\n",
    "T = df[df.columns[-3:]].to_numpy()\n",
    "T = T.T\n",
    "X = df[df.columns[:-3]].to_numpy()\n",
    "X = numpy.vstack((numpy.ones(X.shape[0]),X.T)) # add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main script\n",
    "\n",
    "# hyperparameters\n",
    "eta = 0.0001\n",
    "B = 32\n",
    "epochs = 10000\n",
    "k = 100\n",
    "\n",
    "# init weights\n",
    "W1 = numpy.random.uniform(size=(k+1,X.shape[0])) # first layer weights\n",
    "W2 = numpy.random.uniform(size=(T.shape[0],k+1)) # second layer weights\n",
    "\n",
    "# run stochastic gradient descent\n",
    "W1_sgd, W2_sgd, loss_arr_sgd = stochastic_gradient_descent(X, T, W1.copy(), W2.copy(), B, eta, epochs)\n",
    "    \n",
    "# run gradient descent\n",
    "W1_gd, W2_gd, loss_arr_gd = gradient_descent(X, T, W1.copy(), W2.copy(), eta, epochs)\n",
    "    \n",
    "# plot loss functions for stochastic gradient descent and gradient descent\n",
    "x = numpy.arange(0, epochs)\n",
    "plt.loglog(x,loss_arr_sgd,'-')\n",
    "plt.loglog(x,loss_arr_gd,'-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
