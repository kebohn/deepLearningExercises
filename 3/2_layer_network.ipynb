{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "continuing-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "academic-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x):\n",
    "    return (numpy.cos(3 * x) + 1) / 2\n",
    "\n",
    "def gaussian(x):\n",
    "    return numpy.exp(-0.25 * x ** 2)\n",
    "\n",
    "def polynomial(x):\n",
    "    return (x ** 5 + 3 * x ** 4 - 11 * x ** 3 - 27 * x ** 2 + 10 * x + 64) / 100 \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + numpy.exp(-x))\n",
    "\n",
    "def loss_function(y, t): # L2 Loss\n",
    "    return numpy.mean((y - t) ** 2)\n",
    "\n",
    "def network_output(x, w_1, w_2):\n",
    "    h = sigmoid(w_1.dot(x))\n",
    "    h[0] = 1. # bias\n",
    "    y = w_2.dot(h)\n",
    "    return y, h\n",
    "\n",
    "def compute_gradient_layer_1(x, w, h, y, t):\n",
    "    tmp = numpy.outer(w, y - t) * h * (1 - h)\n",
    "    return tmp.dot(x.T) / len(x)\n",
    "\n",
    "def compute_gradient_layer_2(h, y, t):\n",
    "    return numpy.mean((y - t) * h, axis=1)\n",
    "\n",
    "def gradient_descent(x, t, w_1, w_2, eta, epochs):\n",
    "    loss_array = numpy.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        y, h = network_output(x, w_1, w_2)\n",
    "        w_1 -= eta * compute_gradient_layer_1(x, w_2, h, y, t)\n",
    "        w_2 -= eta * compute_gradient_layer_2(h, y, t)\n",
    "        y, h = network_output(x, w_1, w_2)\n",
    "        loss_array[epoch] = loss_function(y, t)\n",
    "    \n",
    "    return w_1, w_2, loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impaired-happening",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "random() got an unexpected keyword argument 'low'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a9def01667a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;31m# add bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.random\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: random() got an unexpected keyword argument 'low'"
     ]
    }
   ],
   "source": [
    "# define number of samples\n",
    "n = 200\n",
    "\n",
    "# datasets\n",
    "x_1 = numpy.random.uniform(low=-2, high=2, size=(2,n))\n",
    "x_1[0] = 1. # add bias\n",
    "t_1 = cosine(x_1[1,:])\n",
    "\n",
    "x_2 = numpy.random.uniform(low=-2, high=2, size=(2,n))\n",
    "x_2[0] = 1. # add bias\n",
    "t_2 = gaussian(x_2[1,:])\n",
    "\n",
    "x_3 = numpy.random.uniform(low=-4.5, high=3.5, size=(2,n))\n",
    "x_3[0] = 1. # add bias\n",
    "t_3 = polynomial(x_3[1,:])\n",
    "\n",
    "# init weights\n",
    "k = 20 # number of hidden neurons\n",
    "w_1 = numpy.random.uniform(low=-10, high=10, size=(k+1,2)) # first layer weights\n",
    "w_2 = numpy.random.uniform(low=-10, high=10, size=(k+1)) # second layer weights\n",
    "\n",
    "# learning rate\n",
    "eta = 0.1\n",
    "\n",
    "# number of epochs\n",
    "epochs = 50000\n",
    "\n",
    "# run gradient descent for x_1 dataset\n",
    "w_x_1_1, w_x_1_2, loss_1 = gradient_descent(x_1, t_1, numpy.copy(w_1), numpy.copy(w_2), eta, epochs)\n",
    "\n",
    "# run gradient descent for x_3 dataset\n",
    "w_x_2_1, w_x_2_2, loss_2 = gradient_descent(x_2, t_2, numpy.copy(w_1), numpy.copy(w_2), eta, epochs)\n",
    "\n",
    "# run gradient descent for x_3 dataset\n",
    "w_x_3_1, w_x_3_2, loss_3 = gradient_descent(x_3, t_3, numpy.copy(w_1), numpy.copy(w_2), eta, epochs)\n",
    "\n",
    "# compute the network outputs\n",
    "x_1_new = numpy.arange(-2,2,0.001)\n",
    "x_1_new = numpy.vstack((numpy.ones(x_1_new.shape), x_1_new)) # add bias\n",
    "y_1 = network_output(x_1_new, w_x_1_1, w_x_1_2)[0]\n",
    "\n",
    "x_2_new = numpy.arange(-2,2,0.001)\n",
    "x_2_new = numpy.vstack((numpy.ones(x_2_new.shape), x_2_new)) # add bias\n",
    "y_2 = network_output(x_2_new, w_x_2_1, w_x_2_2)[0]\n",
    "\n",
    "x_3_new = numpy.arange(-4.5,3.5,0.001)\n",
    "x_3_new = numpy.vstack((numpy.ones(x_3_new.shape), x_3_new)) # add bias\n",
    "y_3 = network_output(x_3_new, w_x_3_1, w_x_3_2)[0]\n",
    "\n",
    "\n",
    "# plot the graphs\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,figsize=(15,15))\n",
    "ax1.scatter(x_1[1,:], t_1, marker='+')\n",
    "ax1.plot(x_1_new[1,:], y_1,'r-')\n",
    "ax1.title.set_text('Cosine function')\n",
    "ax2.scatter(x_2[1,:], t_2, marker='+')\n",
    "ax2.plot(x_2_new[1,:], y_2,'r-')\n",
    "ax2.title.set_text('Gaussian function')\n",
    "ax3.scatter(x_3[1,:], t_3, marker='+')\n",
    "ax3.plot(x_3_new[1,:], y_3,'r-')\n",
    "ax3.title.set_text('Polynomial function')\n",
    "fig.legend(['approx function','true function'])\n",
    "\n",
    "# plot the loss functions\n",
    "x = numpy.arange(0,epochs)\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3,figsize=(15,15))\n",
    "ax1.plot(x, loss_1)\n",
    "ax1.title.set_text('Loss Cosine function')\n",
    "ax2.plot(x, loss_2)\n",
    "ax2.title.set_text('Loss Gaussian function')\n",
    "ax3.plot(x, loss_3)\n",
    "ax3.title.set_text('Loss Polynomial function')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
