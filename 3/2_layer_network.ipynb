{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greenhouse-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "skilled-hanging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine(x):\n",
    "    return (numpy.cos(3 * x) + 1) / 2\n",
    "\n",
    "def gaussian(x):\n",
    "    return numpy.exp(-0.25 * x ** 2)\n",
    "\n",
    "def polynomial(x):\n",
    "    return (x ** 5 + 3 * x ** 4 - 11 * x ** 3 - 27 * x ** 2 + 10 * x + 64) / 100 \n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + numpy.exp(-x))\n",
    "    \n",
    "def y(x, w):\n",
    "    return w.T.dot(x)\n",
    "\n",
    "def loss_function(y, t): # L2 Loss\n",
    "    return numpy.mean((y - t) ** 2) / 2\n",
    "\n",
    "def compute_gradient_layer_1(x, w, h, y, t):\n",
    "    tmp = (y - t)\n",
    "    print(type(tmp))\n",
    "    \n",
    "    hadamard = w * h * (1 - h)\n",
    "    \n",
    "    print(hadamard)\n",
    "    print(hadamard.shape)\n",
    "    \n",
    "    print(tmp * hadamard)\n",
    "    tmp_2 = np.outer(tmp, x)\n",
    "    return 2 * numpy.mean(tmp_2) \n",
    "\n",
    "def compute_gradient_layer_2(h, y, t):\n",
    "    return 2 * numpy.mean((y - t) * h)\n",
    "\n",
    "def gradient_descent(x, t, w_1, w_2, eta, epochs):\n",
    "    loss_array = numpy.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        h = sigmoid(w_1.dot(x))\n",
    "        Y = y(h, w_2)\n",
    "        w_1 -= eta * compute_gradient_layer_1(x, w_2, h, Y, t)\n",
    "        w_2 -= eta * compute_gradient_layer_2(h, Y, t)\n",
    "        h = sigmoid(w_1.dot(x))\n",
    "        Y = y(h, w_2)\n",
    "        loss_array[epoch] = loss_function(Y, t)\n",
    "    \n",
    "    return w_1, w_2, loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "current-morning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[ 1.27443080e+000 -6.93369360e-207  5.80999360e-076 -2.28544986e-029\n",
      "  1.32509810e-018  0.00000000e+000 -1.36909801e-055  0.00000000e+000\n",
      " -1.74525412e-007  0.00000000e+000  0.00000000e+000]\n",
      "(11,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (715,) (11,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-68aa36c71e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# run gradient descent for x_1 dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mw_x_1_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_x_1_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# plot the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-66e29c0c2dcc>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(x, t, w_1, w_2, eta, epochs)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mw_1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcompute_gradient_layer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mw_2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcompute_gradient_layer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-66e29c0c2dcc>\u001b[0m in \u001b[0;36mcompute_gradient_layer_1\u001b[0;34m(x, w, h, y, t)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhadamard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhadamard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mtmp_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (715,) (11,) "
     ]
    }
   ],
   "source": [
    "# define number of samples\n",
    "d = numpy.random.randint(low=20, high=2000)\n",
    "\n",
    "# datasets\n",
    "x_1 = numpy.random.uniform(low=-2, high=2, size=(d+1))\n",
    "x_1[0] = 1. # add bias\n",
    "t_1 = cosine(x_1)\n",
    "\n",
    "x_2 = numpy.random.uniform(low=-2, high=2, size=(d+1))\n",
    "x_2[0] = 1. # add bias\n",
    "t_2 = gaussian(x_2)\n",
    "\n",
    "x_3 = numpy.random.uniform(low=-4.5, high=3.5, size=(d+1))\n",
    "x_3[0] = 1. # add bias\n",
    "t_3 = polynomial(x_3)\n",
    "\n",
    "# init weights\n",
    "k = 10 # number of hidden neurons\n",
    "w_1 = numpy.random.uniform(low=-10, high=10, size=(k+1,d+1)) # first layer weights\n",
    "w_1[0,:] = 0 # set to zero (bias)\n",
    "w_2 = numpy.random.uniform(low=-10, high=10, size=(k+1)) # second layer weights\n",
    "\n",
    "# learning rate\n",
    "eta = 0.02\n",
    "\n",
    "# number of epochs\n",
    "epochs = 1\n",
    "\n",
    "# run gradient descent for x_1 dataset\n",
    "w_x_1_1, w_x_1_2, loss_1 = gradient_descent(x_1, t_1, numpy.copy(w_1), numpy.copy(w_2), eta, epochs)\n",
    "\n",
    "# plot the graph\n",
    "\n",
    "# run gradient descent for x_3 dataset\n",
    "#w_x_2_1, w_x_2_2, loss_2 = gradient_descent(x_2, t_2, numpy.copy(w_1), numpy.copy(w_2), eta, epochs)\n",
    "\n",
    "# plot the graph\n",
    "\n",
    "# run gradient descent for x_3 dataset\n",
    "#w_x_3_1, w_x_3_2, loss_3 = gradient_descent(x_3, t_3, numpy.copy(w_1), numpy.copy(w_2), eta, epochs)\n",
    "\n",
    "# plot the graph\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
